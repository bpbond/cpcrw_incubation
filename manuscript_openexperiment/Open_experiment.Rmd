---
title: 'Running an open experiment: transparency and reproducibility in soil and ecosystem
  science'
author: "Ben Bond-Lamberty^1^, A. Peyton Smith^2^, Vanessa Bailey^2^"
date: "February 2, 2016"
output: word_document
---
^1^Joint Global Change Research Institute, DOE Pacific Northwest National Laboratory, College Park, MD USA; 
bondlamberty@pnnl.gov

^2^Biological Sciences Division, Pacific Northwest National Laboratory, Richland, WA USA

#### Abstract

Researchers in soil and ecosystem science, and almost every other field, as increasingly being pushed--by funders, journals, governments, and themselves--to ensure transparency and reproducibility of their work. A key part of this effort is a move towards *open data* as a way to fight post-publication data loss, improve data and code quality, and enable powerful meta- and cross-disciplinary analyses. Here we describe a recent "open experiment", in which we documented every aspect of a soil incubation experiment online. This involved making raw data, scripts, diagnostics, final analyses, and manuscripts publicly available in real time. We found that using tools such as version control, issue tracking, and open-source statistical software improved data integrity, accelerated our team's communications, and ensured transparency. There are many avenues to improve scientific reproducibility and data availability, of which is this only one example, and it is not an approach suited for every experiment or situation. Nonetheless, we encourage the communities in our respective fields to consider its advantages, and to lead rather than follow with respect to scientific reproducibility, transparency, and data availability.

#### Introduction

Science is becomingly increasingly collaborative and data-intensive {Adams, 2012 #4348}. At the same time, a wide variety of factors are encouraging, at times pushing, scientists to increase data access and use "best practices" in dealing with data and code {Hart, 2015 #4292;Wilson, 2014 #4337}.

Journals are adopting increasingly stringent data access and deposition policies, e.g. Scientific Data[^1], PLOS ONE[^2], and Science[^3]. These policies generally share common assumptions and goals: maximizing access to scientific data; encouraging deposition into structured repositories as opposed to journal supplementary information; and specifying that it is not acceptable for authors to be solely responsible for ensuring data access. Funders are moving in this direction as well, with organizations such as the U.S. National Science Foundation[^4] and the U.K. Wellcome Trust[^5], along with many others, requiring explicit data management plans, unfettered reasonable access to primary data, and use of established repositories.

[^1]: http://www.nature.com/sdata/data-policies
[^2]: http://journals.plos.org/plosone/s/data-availability
[^3]: http://www.sciencemag.org/authors/science-editorial-policies#data-deposition
[^4]: http://www.nsf.gov/bfa/dias/policy/dmp.jsp
[^5]: http://www.wellcome.ac.uk/About-us/Policy/Spotlight-issues/Data-sharing/Data-management-and-sharing/index.htm

Finally, growing numbers of scientists are pushing for open science and data on moral/political grounds, as well as purely scientific ones, in particular ensuring reproducibility {Stodden, 2011 #4340} and enabling larger synthetic activities. Such analyses {Wolkovich, 2012 #3303} are made possible by the assembly of large, internally consistent data sets. Examples in ecology, soil science, and biogeosciences include BAAD {Falster, 2015 #4209}, TRY {Kattge, 2011 #3064}, FLUXNET {Baldocchi, 2001 #2883}, and SRDB {Bond-Lamberty, 2010 #2320}.

We briefly discuss what we see as the primary arguments for data sharing and openness, before detailing a recent "open experiment" example.

**Reproducibility**

Reproducibility of experimental results is at the heart of science {Kuhn, 1962 #4341}. Too often, however, it is impossible to attempt, because sufficient details and data are not publicly available. For example, surveying the 2000-2014 biomedical literature, Iqbal et al. {, 2016 #4334} found that none of 441 randomly-chosen studies provided raw data, and only one provided full protocols. The issue of reproducibility in ecology (and its many related fields) has been raised and debated for years {Cassey, 2006 #4349} but ecological, soil science, and global change journals differ widely in their data deposition requirements.

As code and data have become increasingly intertwined, the availability of the former has become a fundamental problem as well. Modeling groups have worked to improve reproducibility and archiving praactices {Thornton, 2005 #2310}. Scientists in all fields are increasingly building and using software in their work, however, often without strong training in this area {Wilson, 2014 #4337}. In addition to open-source data analysis languages such as R {R Development Core Team, 2016 #4314}, scientific workflow systems such as Kepler (https://kepler-project.org) or Taverna (http://www.taverna.org.uk) record information about the data processing, analytical process and decisions, and statistical analysis. Providing open code does not magically produce bug-free code, or mistake free analyses {Easterbrook, 2014 #4137}. But it does encourage authors to invest the time upfront to clean up their code, data, and documentation when a paper is written, rather than deferring this task (often, forever).

**Data loss**

Vines et al. {, 2014 #4130} published a shocking result, based on surveying 516 biology articles from 2 to 22 years old: the odds of a data set being available fell by 17% each year, and the chances that the contact author's email address still worked declined by 7% per year. Similarly, Reichman et al. {, 2011 #2862} estimated that less than 1% of ecological data collected is made available after publication, and noted, as an example, that much current and historical data relevant to the recent *Deepwater Horizon* oil spill are already inaccessible or lost {Reichman, 2011 #2862}. In global change ecology, Wolkovich et al. {, 2012 #3303} reported that only ~10% of raw data sets were eventually shared in their attempt to conduct a meta-analysis {Wolkovich, 2012 #3331}. 

Data loss hits ecosystem, soil, and global change ecology particularly hard, as global to local climate changes make ecological data effectively irreproducible {Wolkovich, 2012 #3303}: we can never remeasure exactly the same system state. A subtler data loss issue is the "file drawer problem" {Rosenthal, 1979 #2291}, where unpublished but potentially valuable data are lost fovever. Scientists' use of strong and consistent data curation practices {Hart, 2015 #4292}{RÃ¼egg, 2014 #3886} can mitigate the problem, but all our anecdotal experience and quantitative studies {Vines, 2014 #4130} suggest that in the long term, data cannot be reliably preserved by individual researchers. This in turns means that the role of established, structured data and code repositories is critical.

TODO - VLB: Supplementary Information sections have become repositories of data and of non-core analyses. How does our open experiment replace (improve upon?) SI? Thoughts?  - journals not ready, and don't want {Cassey, 2006 #4349}.

**Institutional and social trust** (TODO)

Finally, there are longer-term issues of trust to consider with respect to reproducibility and openness. 

Governmental efforts to have the results of federally funded scientific research made available to the public, industry, and across the scientific community {Holdren, 2013 #4344}. 

* Trust in correctness of science: Potential to increase public trust in research results (e.g. climate change)
* Trust in utility of science: Why should the public fund scientists if the latter aren't producing demonstrably replicable and reusable results?

TODO: mention that of course authors have some rights. But what is being denied, increasingly, is that "the intellectual property rights of publishing ecologists" {Cassey, 2006 #4349} take precedence over all other factors, at least as far as publicly-funded research is concerned.


#### An open experiment: one example

In early 2015, we planned a laboratory incubation experiment to characterize the chemical and biological properties of sub-Arctic, active layer soils subjected to changes in temperature and moisture. In this experiment, we would measure greenhouse gas fluxes from soil cores over 100 days, and measure the cores' physical, chemical, and biological characteristics under temperature and moisture changes. This required (i) a multidisciplinary team that was not located in one time zone; (ii) integrating a variety of different data; (iii) performing quality control and diagnostics rapidly, so if e.g. instrument problems arose we would lose only the minimum amount of time and data; (iv) tightly integrating data, statistical analyses, and manuscript results.

We used GitHub, a web-based Git repository hosting service, to store our data, scripts, and manuscripts. "Git" is a popular, free, and open source version control software: it tracks changes (when, what, by whom) made in a "repository", a collection of folders and files. Many scientific and other users of Git use the "GitHub" web service, as it offers excellent performance and a wide variety useful additional functinality, particularly for teams or collaborative projects.

The design of our repository is shown in **Figure 1**, and the repository itself can be found at https://github.com/bpbond/cpcrw_incubation. It consists of a series of scripts that comprise a *pipeline*, start with raw data and ending with final analyses, figures, and manuscripts.

This system had a number of characteristics:

* The entire data processing and analytical system is online and documented. It is written in R {R Development Core Team, 2016 #4314}, an open-source and widely used language and environment for statistical computing and graphics.
* The version control system let us make incremental changes, work out problems, look at histories (i.e., who made what change when).
* A issue tracker let us discuss problems, reference changes in the repository, create to-do lists, and assign responsibilities.
* Information webpage on the experiment and broader project.
* Manuscripts as well! (Including this one.) Results are linked directly into the manuscripts, so that if a result changes, it is updated automatically in the text. Tools such as R Markdown[^7] have made this process much easier to build and maintain.

[^7]: http://rmarkdown.rstudio.com

Advantages:

* The public nature of the repository encouraged us to flesh out documentation, use clean and clear coding, and think about the longer-term ramifications of decisions.
* Real-time diagnostics (**Figure 2**) let our team, program managers, and interested other parties see at a glance the progress of the experiment.
* The issue tracker helped our team--which was not physically located all in one place--to communicate, discuss, and track "issues" (i.e., problems or questions that arose).
* Investing the effort to set up a software pipeline before, not after, the experiment was performed meant that we could reliably and easily identify and diagnose problems.

#### Conclusions

Transparency is no magic panacea ensuring reproducibility {Easterbrook, 2014 #4137}, and may raise genuine concerns about, for example, protecting scientists from harassment {Lewandowsky, 2016 #4342}. Scientists also frequently cite other concerns about data sharing, for example being "scooped" not receiving sufficient credit, and time constraints {Wolkovich, 2012 #3303}. We cannot easily dismiss of these these: for example, does our community adequately credit researchers who contribute to global databases that subsequently produce high-impact papers? Such meta-analyses rely on the collection of primary data, and it is critical that field and experimental researchers efforts are adequately valued and cited {Kueffer, 2011 #4335}. We also recognize that, in general, our professional and career incentives do not yet align well with "an open research culture" {Nosek, 2015 #4343}--that is, scientists do not receive 'proper' credit for datasets, relative to publications, even when the former may be much more broadly valueable.

The model we present here--a public repository updated throughout the experiment, analysis, and publication process--is far from perfect. First, there are a number of "best practices" of scientific computing {Wilson, 2014 #4337} that we did not employ: for example, there was no unit testing to ensure module correctness, nor an automated build tool automating the workflow (i.e., the individual scripts all have to be run manually). Second, it may not not be appropriate for applicable to all study types, even just in our professional fields of soil, ecosystem, and global change science. (And it certainly is not appropriate for sensitive human-subject data.) Third, Git and GitHub are designed for working with code, not data, although this situation continues to improve on a technical level[^6].

[^6]: https://github.com/blog/1885-better-word-highlighting-in-diffs

Nonetheless (TODO: it's a lot better than nothing, and offers instrument-to-final product reproducibility).

It doesn't have to be adopted wholesale: for example, the use of an issue tracker alone might be valuable for some groups and improve their science (though not reproducibility).

This is example, we hope, of the impact of individual scientists' decisions and practices {Wolkovich, 2012 #3303}.

We encourage the communities in our respective fields to consider its advantages, and to lead rather than follow with respect to scientific reproducibility, transparency, and data availability.



**References**


**Figure 1.** The 'repository' containing all code and data of a recent soil incubation experiment. From top to bottom: raw data from a lab analyzer is uploaded; an initial script processes raw data into a standardized, summarized form; a second script looks for inconsistencies and outliers in the data; a third computes final data products, summaries, statistics, and figures; and a final one integrates these products to produce a submission-ready manuscript. (This schematic slightly simplifies the actual repository structure.) The repository is publicly available at https://github.com/bpbond/cpcrw_incubation, and throughout the experiment showed real-time updated diagnostics summarizing progress.

![Repository and workflow](figure1.png)

**Figure 2.** Two of the many diagnostic figures that were produced, in real time, as the incubation experiment proceeded. They show (top) the number of replicate measurements made on each soil core, by date of measurement, and (bottom) measurements with a data mismatch problem, indicating a data entry or other error (in the figure no errors, which would be in red, are extant). A wide range of similar diagnostic graphics (see https://github.com/bpbond/cpcrw_incubation#current-diagnostics) and statistics let us quickly identify problems or interesting patterns in the data.

![Sample diagnostics](figure2.png)
